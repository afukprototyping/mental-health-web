# -*- coding: utf-8 -*-
"""NEREnhanced.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1v8nRQ3IYppDMvDYFKzxN9QdTYV4QwnMW
"""

import re
import os
import pickle
import pandas as pd
import numpy as np
import torch
from transformers import AutoTokenizer, AutoModel
from sklearn.preprocessing import normalize
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report, accuracy_score
from collections import defaultdict
from typing import Dict, List, Set, Tuple

# --- KONFIGURASI ---
MODEL_NAME = "indolem/indobert-base-uncased"
# Ganti path ini jika file Anda tidak berada di /content/
DATA_PATH = "/content/df (1).csv"
SAVE_DIR = "model"
os.makedirs(SAVE_DIR, exist_ok=True)
PKL_PATH = os.path.join(SAVE_DIR, "preprocessed_components.pkl")
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
RANDOM_STATE = 42

BEST_PARAMS = {
    'n_estimators': 226,
    'learning_rate': 0.0948329850116092,
    'max_depth': 4,
    'subsample': 0.7280247373192,
    'min_samples_split': 12,
    'min_samples_leaf': 2,
    'random_state': RANDOM_STATE
}

def minimal_preprocessing(text):
    """Membersihkan karakter khusus dan whitespace"""
    text = str(text)
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

class KeywordNER:
    def __init__(self):
        # KAMUS KATA KUNCI YANG DIPERKAYA (Konsisten dengan kode acuan)
        self.mental_health_keywords = {
            'anxiety': ['cemas', 'gelisah', 'khawatir', 'panik', 'takut', 'nervous', 'anxiety', 'deg-degan', 'jantung berdebar', 'keringat dingin', 'tegang', 'was-was', 'overthinking', 'kecemasan', 'gangguan kecemasan'],
            'depression': ['depresi', 'sedih', 'murung', 'putus asa', 'hopeless', 'down', 'tidak bersemangat', 'malas', 'lelah', 'capek', 'kosong', 'hampa', 'tidak ada motivasi', 'kehilangan minat', 'bad mood'],
            'stress': ['stres', 'stress', 'tekanan', 'beban', 'pusing', 'overwhelmed', 'kewalahan', 'terbebani', 'lelah mental', 'burnout', 'jenuh', 'frustrasi', 'terpuruk', 'tertekan'],
            'sleep_disorder': ['tidur', 'insomnia', 'susah tidur', 'tidak bisa tidur', 'begadang', 'mimpi buruk', 'nightmare', 'bangun malam', 'gelisah tidur', 'mengigau', 'sleepwalking', 'kantuk', 'ngantuk'],
            'adhd': ['adhd', 'hiperaktif', 'hyperactive', 'sulit fokus', 'tidak bisa diam', 'impulsif', 'pelupa', 'ceroboh', 'attention deficit', 'konsentrasi', 'susah berkonsentrasi', 'mudah teralihkan'],
            'autism': ['autis', 'autism', 'spektrum autisme', 'komunikasi sosial', 'interaksi sosial', 'repetitif', 'stimming', 'sensori', 'rutinitas', 'pola perilaku'],
            'bipolar': ['bipolar', 'mood swing', 'mania', 'manik', 'euforia', 'perubahan mood', 'naik turun', 'episode', 'hypomanic'],
            'ptsd': ['trauma', 'ptsd', 'flashback', 'kenangan buruk', 'terganggu', 'kekerasan', 'pelecehan', 'abuse', 'shock', 'terpukul'],
            'eating_disorder': ['makan', 'nafsu makan', 'anoreksia', 'bulimia', 'binge eating', 'diet berlebihan', 'tidak mau makan', 'muntah', 'body image'],
            'addiction': ['kecanduan', 'adiksi', 'ketergantungan', 'narkoba', 'alkohol', 'rokok', 'game online', 'media sosial', 'gambling', 'judi'],
            'ocd': ['ocd', 'obsesi', 'kompulsi', 'ritual', 'berulang-ulang', 'tidak bisa berhenti', 'terus menerus', 'terpaksa melakukan'],
            'schizophrenia': ['skizofrenia', 'halusinasi', 'delusi', 'waham', 'mendengar suara', 'melihat sesuatu', 'paranoid', 'curiga berlebihan']
        }
        self.demographic_keywords = {
            'child': ['anak', 'balita', 'bocah', 'kecil', 'sd', 'tk'],
            'teen': ['remaja', 'abg', 'smp', 'sma', 'teenager', 'adolescent'],
            'adult': ['dewasa', 'kuliah', 'kerja', 'karir', 'menikah'],
            'elderly': ['lansia', 'tua', 'lanjut usia', 'pensiunan']
        }
        self.severity_keywords = {
            'mild': ['ringan', 'sedikit', 'agak', 'kadang-kadang', 'sesekali'],
            'moderate': ['sedang', 'cukup', 'lumayan', 'sering'],
            'severe': ['parah', 'berat', 'sangat', 'ekstrem', 'selalu', 'terus menerus']
        }

    def extract_entities(self, text):
        text_lower = text.lower()
        entities = {'mental_health': [], 'demographic': [], 'severity': []}

        for cat, words in self.mental_health_keywords.items():
            entities['mental_health'].extend([(cat, w) for w in words if w in text_lower])
        for cat, words in self.demographic_keywords.items():
            entities['demographic'].extend([(cat, w) for w in words if w in text_lower])
        for cat, words in self.severity_keywords.items():
            entities['severity'].extend([(cat, w) for w in words if w in text_lower])
        return entities

    def get_label_keywords(self, df):
        label_keywords = defaultdict(set)
        for label in df['Label'].unique():
            label_texts = df[df['Label'] == label]['Pertanyaan_Clean'].tolist()
            for text in label_texts:
                entities = self.extract_entities(text)
                for cat, kw in entities['mental_health'] + entities['demographic'] + entities['severity']:
                    label_keywords[label].add(kw)
        return dict(label_keywords)

class IndoBERTFeatureExtractor:
    def __init__(self, model, tokenizer, device, max_length=256):
        self.model = model
        self.tokenizer = tokenizer
        self.device = device
        self.max_length = max_length

    def extract_features(self, texts, batch_size=8):
        """Ekstraksi embedding [CLS] dari IndoBERT (dinormalisasi)"""
        all_features = []
        self.model.eval()
        for i in range(0, len(texts), batch_size):
            batch_texts = texts[i:i + batch_size]
            encoded = self.tokenizer(
                batch_texts, padding=True, truncation=True, max_length=self.max_length, return_tensors='pt'
            ).to(self.device)

            with torch.no_grad():
                outputs = self.model(**encoded)
                # Menggunakan [CLS] token (index 0)
                features = outputs.last_hidden_state[:, 0, :].cpu().numpy()
            all_features.append(features)

        features = np.vstack(all_features)
        # Normalisasi L2 untuk fitur BERT
        return normalize(features)

def create_ner_features(texts: List[str], ner_extractor: KeywordNER, label_keywords: Dict[str, Set[str]]) -> np.ndarray:
    """Buat fitur NER berdasarkan Entity Counts dan Keyword Match Scores."""
    ner_features = []
    # Mengambil urutan kategori dari extractor untuk konsistensi vektor fitur
    mh_cats = list(ner_extractor.mental_health_keywords.keys())
    demo_cats = list(ner_extractor.demographic_keywords.keys())
    sev_cats = list(ner_extractor.severity_keywords.keys())
    # Mengurutkan label untuk konsistensi fitur skor kecocokan
    labels_ordered = sorted(label_keywords.keys())

    for text in texts:
        entities = ner_extractor.extract_entities(text)
        feature_vector = []
        text_lower = text.lower()

        # 1. Entity Counts
        counts = defaultdict(int)
        for category, _ in entities['mental_health']: counts[category] += 1
        for category in mh_cats: feature_vector.append(counts[category])

        counts = defaultdict(int)
        for category, _ in entities['demographic']: counts[category] += 1
        for category in demo_cats: feature_vector.append(counts[category])

        counts = defaultdict(int)
        for category, _ in entities['severity']: counts[category] += 1
        for category in sev_cats: feature_vector.append(counts[category])

        # 2. Keyword Match Scores
        for label in labels_ordered:
            keywords = label_keywords.get(label, set())
            # Match score = jumlah keyword yang cocok / total keyword yang ada di label
            match_score = sum(1 for kw in keywords if kw in text_lower) / max(len(keywords), 1)
            feature_vector.append(match_score)

        ner_features.append(feature_vector)
    return np.array(ner_features)

def execute_full_pipeline(data_path, pkl_path, model_name, device, best_params):
    """Fungsi utama untuk menjalankan seluruh pipeline."""

    # --- PHASE 1: PREPROCESSING & FEATURE EXTRACTION (Save to PKL) ---
    if not os.path.exists(pkl_path):
        print("\n=== FASE 1: EKSTRAKSI FITUR DAN PENYIMPANAN ===")

        # 1. Load Data
        df = pd.read_csv(data_path)
        df["Pertanyaan_Clean"] = df["Pertanyaan"].apply(minimal_preprocessing)
        df["Jawaban_Clean"] = df["Jawaban"].apply(minimal_preprocessing)
        df = df.drop_duplicates(subset=["Pertanyaan_Clean"], keep="first")
        print(f"🔹 Dataset loaded and cleaned: {df.shape[0]} rows")

        # 2. Load Model
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        bert_model = AutoModel.from_pretrained(model_name)
        bert_model.to(device)
        print(f"🔹 IndoBERT loaded on {device}")

        # 3. Extract Keywords & BERT Features
        ner_extractor = KeywordNER()
        label_keywords = ner_extractor.get_label_keywords(df)
        print("🔹 Kata Kunci NER per Label diekstrak.")

        extractor = IndoBERTFeatureExtractor(bert_model, tokenizer, device)
        features_bert = extractor.extract_features(df["Pertanyaan_Clean"].tolist())
        print(f"🔹 IndoBERT Embeddings diekstrak. Shape: {features_bert.shape}")

        # 4. Save to PKL
        with open(pkl_path, "wb") as f:
            pickle.dump({
                "questions": df["Pertanyaan_Clean"].tolist(),
                "answers": df["Jawaban_Clean"].tolist(),
                "labels": df["Label"].tolist() if "Label" in df.columns else None,
                "features_bert": features_bert,
                "label_keywords": label_keywords
            }, f)
        print(f"✅ FASE 1 SELESAI. File disimpan di: {pkl_path}")

    # --- PHASE 2: LOAD PKL, COMBINE FEATURES, AND TRAIN MODEL ---
    print("\n=== FASE 2: PEMUATAN DATA, GABUNGAN FITUR & PELATIHAN ===")

    # 1. Load Data from PKL
    with open(pkl_path, "rb") as f:
        data = pickle.load(f)

    X_bert = data["features_bert"]
    Y_labels = np.array(data["labels"])
    texts_clean = data["questions"]
    LABEL_KEYWORDS_STORED = data["label_keywords"]
    print(f"🔹 Data dimuat dari PKL. Jumlah sampel: {X_bert.shape[0]}")

    # 2. Compute NER Features and Combine
    ner_extractor = KeywordNER()
    X_ner = create_ner_features(texts_clean, ner_extractor, LABEL_KEYWORDS_STORED)
    X_combined = np.concatenate([X_bert, X_ner], axis=1)

    print(f"🔹 Fitur NER dihitung. Shape: {X_ner.shape}")
    print(f"🔹 Fitur Gabungan (BERT + NER) final shape: {X_combined.shape}")

    # 3. Encoding Label & Split Data
    le = LabelEncoder()
    y_encoded = le.fit_transform(Y_labels)

    X_train, X_test, y_train, y_test = train_test_split(
        X_combined, y_encoded, test_size=0.2, random_state=BEST_PARAMS['random_state'], stratify=y_encoded
    )

    # 4. Pelatihan Model
    print("⚙️ Melatih Gradient Boosting Classifier dengan Best Parameters...")
    gb_model = GradientBoostingClassifier(**best_params)
    gb_model.fit(X_train, y_train)

    # 5. Evaluasi
    y_pred = gb_model.predict(X_test)
    acc = accuracy_score(y_test, y_pred)
    target_names = le.classes_

    print("\n==============================================")
    print(f"✅ PELATIHAN SELESAI.")
    print(f"   Akurasi pada Data Uji: {acc:.4f}")
    print("==============================================")
    print("Classification Report:\n", classification_report(y_test, y_pred, target_names=target_names))

    # Simpan model final dan LabelEncoder
    with open(os.path.join(SAVE_DIR, "final_gb_model.pkl"), "wb") as f:
        pickle.dump({'model': gb_model, 'label_encoder': le, 'ner_keywords': LABEL_KEYWORDS_STORED}, f)
    print("Model klasifikasi final disimpan.")


# Panggil fungsi utama
execute_full_pipeline(DATA_PATH, PKL_PATH, MODEL_NAME, DEVICE, BEST_PARAMS)







