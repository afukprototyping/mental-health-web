# -*- coding: utf-8 -*-
"""journal_model

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DYYPVKDoIPxesNBvtFjzIwVwzVf6KhPB
"""

!pip install symspellpy
!pip install huggingface_hub[hf_xet]

import pandas as pd
import numpy as np
import re
import torch
from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer, AutoModel
from symspellpy import SymSpell, Verbosity
from sklearn.model_selection import train_test_split
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, roc_curve, f1_score
from sklearn.preprocessing import LabelEncoder
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm
import warnings
import joblib
import os
from datetime import datetime
import json

warnings.filterwarnings('ignore')

class MentalHealthClassificationPipeline:

    def __init__(self, indobert_model="indobenchmark/indobert-base-p1", max_length=128):
        self.indobert_model = indobert_model
        self.max_length = max_length
        self.feature_extractor = None
        self.gb_classifier = None
        self.label_encoder = LabelEncoder()
        self.sym_spell = None
        self.is_fitted = False
        self.best_params = None
        self.training_history = {}

        self.model_dir = "saved_models"
        self.results_dir = "results"
        os.makedirs(self.model_dir, exist_ok=True)
        os.makedirs(self.results_dir, exist_ok=True)

        print("‚úÖ MentalHealthClassificationPipeline initialized")

    def setup_spell_checker(self, dictionary_path=None):
        print("Setting up spell checker...")

        self.sym_spell = SymSpell(max_dictionary_edit_distance=2, prefix_length=7)
        if dictionary_path and os.path.exists(dictionary_path):
            try:
                self.sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)
                print("Dictionary loaded successfully")
            except Exception as e:
                print(f"Error loading dictionary: {e}")
        else:
            print("Dictionary file not found, using custom words only")

        custom_words = [
            "gangguan", "mental", "obat", "anak", "sehat", "stres", "stress", "tidur",
            "konsentrasi", "belajar", "kecemasan", "psikolog", "psikiater", "terapi",
            "depresi", "cemas", "adhd", "trauma", "burnout", "anxiety", "panik", "insomnia",
            "bipolar", "skizofrenia", "ocd", "ptsd", "fobia", "autis", "asperger", "emosi",
            "mood", "gelisah", "khawatir", "sedih", "marah", "takut", "panik", "lelah"
        ]
        for word in custom_words:
            self.sym_spell.create_dictionary_entry(word, 1)
        print(f"‚úÖ Added {len(custom_words)} custom words to dictionary")

    def preprocess_text(self, text):
        if pd.isna(text):
            return ""
        text = str(text).lower()
        text = re.sub(r"@\w+|#\w+|https?://\S+", " ", text)
        text = re.sub(r"[^a-zA-Z0-9\s]", " ", text)
        text = re.sub(r"\s+", " ", text).strip()

        if self.sym_spell:
            corrected_words = []
            for word in text.split():
                if len(word) > 2:
                    suggestions = self.sym_spell.lookup(word, Verbosity.CLOSEST, max_edit_distance=2)
                    corrected_words.append(suggestions[0].term if suggestions else word)
                else:
                    corrected_words.append(word)
            return " ".join(corrected_words)
        return text

    def setup_indobert(self):
        print(f"Loading IndoBERT model: {self.indobert_model}")
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"Using device: {self.device}")
        self.tokenizer = AutoTokenizer.from_pretrained(self.indobert_model)
        self.model = AutoModel.from_pretrained(self.indobert_model)
        self.model.to(self.device)
        self.model.eval()
        print("‚úÖ IndoBERT loaded successfully")

    def extract_features(self, texts, batch_size=16):
        if self.model is None:
            self.setup_indobert()
        features = []
        print("Extracting features from IndoBERT...")
        for i in tqdm(range(0, len(texts), batch_size), desc="Extracting features"):
            batch_texts = texts[i:i+batch_size]
            encoding = self.tokenizer(
                batch_texts,
                truncation=True,
                padding=True,
                max_length=self.max_length,
                return_tensors='pt'
            )
            input_ids = encoding['input_ids'].to(self.device)
            attention_mask = encoding['attention_mask'].to(self.device)
            with torch.no_grad():
                outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)
                cls_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()
                features.extend(cls_embeddings)
        return np.array(features)

    def train(self, df, text_column='twt', label_column='label',
              test_size=0.2, val_size=0.1, dictionary_path=None):

        print("üöÄ Starting simplified training pipeline...")

        self.setup_spell_checker(dictionary_path)
        self.setup_indobert()

        print("üîÑ Preprocessing data...")
        df_clean = df.copy()
        df_clean['text_clean'] = df_clean[text_column].apply(self.preprocess_text)
        df_clean = df_clean[df_clean['text_clean'].str.len() > 0]
        df_clean['label_encoded'] = self.label_encoder.fit_transform(df_clean[label_column])

        print(f"üìä Total samples: {len(df_clean)}")

        X_temp, X_test, y_temp, y_test = train_test_split(
            df_clean['text_clean'].tolist(),
            df_clean['label_encoded'].tolist(),
            test_size=test_size,
            random_state=42,
            stratify=df_clean['label_encoded']
        )

        X_train, X_val, y_train, y_val = train_test_split(
            X_temp, y_temp,
            test_size=val_size/(1-test_size),
            random_state=42,
            stratify=y_temp
        )

        X_train_features = self.extract_features(X_train)
        X_val_features = self.extract_features(X_val)
        X_test_features = self.extract_features(X_test)

        print("‚úÖ Features extracted successfully!")

        # üîß Gunakan parameter terbaik langsung
        self.best_params = {
            'n_estimators': 395,
            'max_depth': 5,
            'learning_rate': 0.031138241527325448,
            'subsample': 0.7932359474353741,
            'max_features': None,
            'min_samples_split': 11,
            'min_samples_leaf': 10
        }

        print("‚úÖ Using predefined best parameters:")
        for k, v in self.best_params.items():
            print(f"   {k}: {v}")

        self.gb_classifier = GradientBoostingClassifier(**self.best_params, random_state=42)
        self.gb_classifier.fit(X_train_features, y_train)

        print("\nüîç Evaluating on test set...")
        y_test_pred = self.gb_classifier.predict(X_test_features)
        y_test_pred_proba = self.gb_classifier.predict_proba(X_test_features)[:, 1]

        test_accuracy = accuracy_score(y_test, y_test_pred)
        test_auc = roc_auc_score(y_test, y_test_pred_proba)
        test_f1 = f1_score(y_test, y_test_pred)

        print(f"‚úÖ Test Accuracy: {test_accuracy:.4f}")
        print(f"‚úÖ Test AUC: {test_auc:.4f}")
        print(f"‚úÖ Test F1: {test_f1:.4f}")
        print("\n" + classification_report(y_test, y_test_pred, target_names=self.label_encoder.classes_))

        self.training_history = {
            'best_params': self.best_params,
            'test_accuracy': test_accuracy,
            'test_auc': test_auc,
            'test_f1': test_f1,
            'label_classes': self.label_encoder.classes_.tolist()
        }

        self.plot_results(y_test, y_test_pred, y_test_pred_proba)
        self.is_fitted = True

        return {
            'test_results': {
                'accuracy': test_accuracy,
                'auc': test_auc,
                'f1': test_f1
            }
        }

    def plot_results(self, y_true, y_pred, y_pred_proba):
        fig, axes = plt.subplots(1, 2, figsize=(12, 5))

        cm = confusion_matrix(y_true, y_pred)
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                    xticklabels=self.label_encoder.classes_,
                    yticklabels=self.label_encoder.classes_,
                    ax=axes[0])
        axes[0].set_title('Confusion Matrix')

        fpr, tpr, _ = roc_curve(y_true, y_pred_proba)
        auc_score = roc_auc_score(y_true, y_pred_proba)
        axes[1].plot(fpr, tpr, label=f'AUC={auc_score:.4f}')
        axes[1].plot([0, 1], [0, 1], 'r--')
        axes[1].set_title('ROC Curve')
        axes[1].legend()

        plt.tight_layout()
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        plt.savefig(os.path.join(self.results_dir, f"evaluation_{timestamp}.png"))
        plt.show()

    def predict(self, texts):
        if not self.is_fitted:
            raise ValueError("Model belum dilatih!")
        if isinstance(texts, str):
            texts = [texts]
        clean_texts = [self.preprocess_text(t) for t in texts]
        features = self.extract_features(clean_texts)
        preds = self.gb_classifier.predict(features)
        probs = self.gb_classifier.predict_proba(features)
        labels = self.label_encoder.inverse_transform(preds)
        return labels, probs

    def save_model(self, model_name="mental_health_final"):
        model_path = os.path.join(self.model_dir, f"{model_name}.joblib")
        model_data = {
            'gb_classifier': self.gb_classifier,
            'label_encoder': self.label_encoder,
            'best_params': self.best_params,
            'training_history': self.training_history,
            'indobert_model': self.indobert_model,
            'max_length': self.max_length,
            'sym_spell': self.sym_spell
        }
        joblib.dump(model_data, model_path)
        print(f"‚úÖ Model saved to {model_path}")
        return model_path

def main():
    df = pd.read_csv('/content/twt - Sheet1.csv')
    print(f"üìä Loaded dataset: {len(df)} samples")

    pipeline = MentalHealthClassificationPipeline()
    results = pipeline.train(
        df=df,
        text_column='twt',
        label_column='label',
        dictionary_path="C:/Users/VICTUS/Downloads/SEC/dictionary.txt"
    )
    pipeline.save_model()

    test_texts = [
        "Saya merasa sangat stres dan tidak bisa tidur",
        "Hari ini saya bahagia sekali",
        "Konsentrasi saya menurun dan sering cemas"
    ]
    preds, probs = pipeline.predict(test_texts)
    for t, p, pr in zip(test_texts, preds, probs):
        print(f"Text: {t}\nPrediction: {p}\nProbabilities: {pr}\n")


if __name__ == "__main__":
    main()



